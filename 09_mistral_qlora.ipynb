{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral Finetune using QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "Finetuning means to tweak an existing model for a particular use case, but for large models, tuning all the parameters of the orginal model is too expensive.\n",
    "There is memory required for the parameters, gradients, and the optimizer.\n",
    "### Concept\n",
    "Quantization at a high level means to split a range into \"buckets\" reducing the memory required for each individual data point.\n",
    "### Solution\n",
    "QLoRA solves this issue by implementing 4 strategies\n",
    "1. 4-bit NormalFLoat  \n",
    "    4-bit NormalFloat is a reduction that takes advantage of how the parameters of a LLM are typically normally distrbuted around 0. It splits the parameter into 16 (4-bits can represent 16 values) *equally-sized* buckets. This is in contrast with *equally-spaced* buckets which can be very sensitive to outliers.\n",
    "2. Double Quantization  \n",
    "    Double Quantization means that we are \"quantizing the quantization constatnts\". In other words, we are using a *block-wise* quantization strategy for each of the 16 buckets. This further prevents the effect of outliers producing an misrepresentative scale.\n",
    "3. Paged Optimizer  \n",
    "    A high level overview of this concept is that the Paged Optimizer allows the GPU and CPU to share memory and transfer pages (of memory) between them, as needed.\n",
    "4. LoRA  \n",
    "    LoRA is a parameters effcient finetuning method that works by adding a small set of trainable parameters to a model, while freezing the original paramenters. On a more technical level, LoRA is implmented through a matrix multiplication trick. If we think of the original weights of the model as an (n x n) matrix, two smaller matricies could represent it if they had the dimension (n x R) @ (R x n) where r << n\n",
    "\n",
    "These 4 components make up strategies used to implement QLoRA allow for finetuning of production level LLMs on consumer grade hardware."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
