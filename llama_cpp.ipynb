{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA.cpp Quickstart \n",
    "\n",
    "[Reference Article: Llama.cpp Tutorial: A Complete Guide to Efficient LLM Inference and Implementation by Zoumana Keita](https://www.datacamp.com/tutorial/llama-cpp-tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pros of Llama.cpp\n",
    "\n",
    "1. Universal Compatibility\n",
    "    - CPU-first design means less complexity and seamless integration into other programming enviornments\n",
    "2. Comprehensive Feature Integration\n",
    "    - provides high-level capabilities, which streamlines development\n",
    "3. Focused Optimization\n",
    "    - single model architecture, allowing precise and effective imporvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main differences between Transformers and Llama architecture\n",
    "\n",
    "1. Pre-normalization (GPT3)\n",
    "2. SwigGLU activation function (PaLM)\n",
    "3. Rotary Embeddings (GPTNeao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify install\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama.cpp Basic Parameters\n",
    "\n",
    "```model_path``` the path to the Llama model file being used  \n",
    "```prompt``` input prompt to the model, which is tokenized and passed to the model  \n",
    "```device``` the device to run the model on (GPU or CPU)  \n",
    "```max_tokens``` max tokens to be generated in the model response  \n",
    "```stop``` a list of strings that will cause the model generation process to stop  \n",
    "```temperature``` [0,1] lower -> more deterministic, higher -> more random  \n",
    "```top_p``` select the most probable tokens whose cumulative probabilty exceeds a given threshold  \n",
    "```echo``` a boolean used to the model includes or does not inlude the original prompt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Instantiate the model\n",
    "my_awesome_llama_model = Llama(model_path=\"./MY_AWESOME_MODEL\")\n",
    "\n",
    "\n",
    "prompt = \"This is a prompt\"\n",
    "max_tokens = 100\n",
    "temperature = 0.3\n",
    "top_p = 0.1\n",
    "echo = True\n",
    "stop = [\"Q\", \"\\n\"]\n",
    "\n",
    "\n",
    "# Define the parameters\n",
    "model_output = my_aweseome_llama_model(\n",
    "       prompt,\n",
    "       max_tokens=max_tokens,\n",
    "       temperature=temperature,\n",
    "       top_p=top_p,\n",
    "       echo=echo,\n",
    "       stop=stop,\n",
    "   )\n",
    "final_result = model_output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
